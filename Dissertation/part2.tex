\chapter{ОПИСАНИЕ АЛГОРИТМОВ}\label{ch:ch2}

Предыдущая глава продемонстрировала критические ограничения существующих подходов и обосновала необходимость интегрированного решения, объединяющего аукционные механизмы, многоагентное обучение и формализм Dec-POMDP. В данной главе мы переходим от теоретического анализа к конкретной реализации, предлагая подробное описание формальной модели задачи и двух основных алгоритмов.

\section{Формализация задачи распределения ресурсов}\label{sec:ch2/sec1}

\subsection{Система как граф}\label{sec:ch2/sec1/subsec1}

Систему управления ресурсами можно представить как граф:

\begin{equation}
	G = (V, E), \quad V = N \cup D, \quad N = \{n_1, n_2, \ldots, n_k\}, \quad D = \{d_1, d_2, \ldots, d_m\}
\end{equation}

где:
\begin{itemize}
	\item $N$ -- множество edge-узлов (поставщиков вычислительных ресурсов);
	\item $D$ -- множество IoT-устройств (потребителей ресурсов);
	\item $E$ -- множество рёбер сетевой топологии;
	\item вес каждого ребра $w(d_i, n_j)$ отражает сетевую задержку между устройством и узлом.
\end{itemize}

Такая графовая модель позволяет естественным образом представить иерархическую структуру облачно-периферийной системы, где различные edge-узлы могут быть на разных уровнях иерархии.

\subsection{Характеристики ресурсов и требования задач}\label{sec:ch2/sec1/subsec2}

Каждый edge-узел $n_j \in N$ характеризуется вектором доступных ресурсов:

\begin{equation}
	\text{Cap}_j = (\text{CPU}_j, \text{MEM}_j, \text{Bandwidth}_j, \text{Storage}_j)
\end{equation}

и текущей загрузкой:

\begin{equation}
	\text{Load}_j(t) = (\text{CPU}_{\text{avail}}(t), \text{MEM}_{\text{avail}}(t), \ldots)
\end{equation}

которая меняется со временем в зависимости от уже выполняемых задач.

Каждое устройство $d_i \in D$ генерирует вычислительные задачи из множества $T_i(t)$ в момент времени $t$, где каждая задача $\tau$ характеризуется:
\begin{itemize}
	\item \textbf{Требования к ресурсам:} $(\text{CPU}(\tau), \text{MEM}(\tau), \text{Bandwidth}(\tau))$;
	\item \textbf{Объём данных:} $\text{DataSize}(\tau)$;
	\item \textbf{Временной дедлайн:} $\text{Deadline}(\tau)$;
	\item \textbf{Приоритет:} $\text{Priority}(\tau) \in [0, 1]$;
	\item \textbf{Тип задачи:} $\text{Type}(\tau)$ (интерактивная, batch, критическая и т.д.).
\end{itemize}

\section{Алгоритм MA-VCG}\label{sec:ch2/sec2}

Моделирование распределения ресурсов можно представить как виртуальный рынок~\cite{gorodetskii}, на котором агенты участвуют в аукционе (см. рис. \ref{img:arch-ma-vcg}). Агенты IoT-устройств выступают как покупатели вычислительных ресурсов, агенты Edge-узлов -- как продавцы, а специальный агент-аукционер координирует весь процесс. 

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm, height=8cm]{arch-ma-vcg}
	\caption{Концептуальная архитектура виртуального рынка}
	\label{img:arch-ma-vcg}
\end{figure}

Муальтиагентный алгоритм на основе механизма аукциона Викри-Кларка-Гровса (\textbf{MA-VCG}) представляет собой адаптацию классического аукционного механизма Викри-Кларка-Гровса (VCG) для многоагентной среды распределения ресурсов в IoT-системах. В отличие от традиционного VCG, работающего в один момент времени, MA-VCG предназначен для повторяющихся аукционов, где параметры функций полезности и стоимости обновляются на основе накопленного опыта.

\textbf{Основная идея:} В каждый временной интервал система выполняет раунд аукциона, в котором реализуется трехфазный протокол:
\begin{enumerate}
	\item Фаза сбора инфорации (Submission Phase): устройства IoT подают заявки с информацией о своих задачах и требуемых ресурсах, а edge-узлы объявляют информацию о своей доступной ёмкости;
	\item Фаза оценки (Evaluation Phase): оценивается полезность и стоимость выполнения задач;
	\item Фаза определения платежей (Payment Phase): аукционер (центральный контроллер) решает оптимальную задачу распределения, на основе этого решения рассчитываются платежи по механизму VCG, задачи назначаются узлам, выполняются платежи.
\end{enumerate}

\subsection{Фаза 1 -- Сбор информации (Submission Phase)}\label{sec:ch2/sec2/subsec1}

На первом фазе агенты устройств $d_{i}$ в момент времени $t$ отправляют заявку на выполнение задач:
\begin{equation}
	\text{Bid}_i(t) = \{\tau_1, \tau_2, \ldots, \tau_{l_i}\}, \quad \text{где } \tau_j = (\text{CPU}, \text{MEM}, \text{Priority}, \ldots)
\end{equation}

С другой стороны каждый узел $n_j$ объявляет доступные ресурсы:
\begin{equation}
	\text{Capacity}_j(t) = (\text{CPU}_{\text{avail}}, \text{MEM}_{\text{avail}}, \text{Bandwidth}_{\text{avail}})
\end{equation}

\subsection{Фаза 2 -- Оценка (Evaluation Phase)}\label{sec:ch2/sec2/subsec2}

На второй фазе оценивается полезность и стоимость выполнения задач. 

\textbf{Функция полезности} является количественной оценкой <<выгоды>> или <<удовлетворенности>> устройства $d_i$ от выполнения его задачи на узле $n_j$, учитывающая базовую ценность задачи, штраф за задержку выполнения и энергетические затраты~\cite{li}:
\begin{equation}
	U_i\left( \tau, n_j, t \right) = V_i(\tau) \cdot \phi\left( \tau, n_j, t \right) - E_{i}(\tau,n_j).
\end{equation}

\textit{Базовая ценность} задачи определяет, сколько <<пользы>> принесет устройству успешное выполнение задачи:
\begin{equation}
	V_j(\tau) = \text{Priority}(\tau) \cdot (\text{CPU}(\tau) + \text{MEM}(\tau)) \cdot \beta_i,
\end{equation}
где $\beta_i$ -- коэффициент значимости для устройства $d_i$.

\textit{Функция штрафа за задержку} определяет, насколько эта польза уменьшается из-за задержки и растет экспоненциально -- это отражает важность соблюдения временных ограничений в IoT-системах:
\begin{equation}
	\phi\left( \tau, n_j, t \right) =  e^{-\lambda \cdot\frac{\text{EstimatedTime}(\tau, n_j, t)}{\text{Deadline}(\tau)}},
\end{equation}
где $\lambda$ -- коэффициент чувствительности к задержке, $\text{EstimatedTime}(\tau, n_j, t)$ -- предполагаемое время выполнения задачи на узле $n_j$.

\textit{Энергозатраты} на выполнение задачи могут быть определены, например, через затраты на передачу данных:
\begin{equation}
	E_i(\tau, n_j) = \text{DataSize}(\tau) \cdot w(d_i, n_j) \cdot e_{\text{trans}}.
\end{equation}

\textbf{Функция стоимости} в свою очередь оценивает затраты, которые несет узел $n_{j}$ при выполнении задачи от устройства $d_{i}$ с учётом вычислительных затрат, коммуникационных расходов и стоимости перегрузки~\cite{qiu}:
\begin{equation}
	C_j\left( \tau, d_i, t \right) = C_{comp}(\tau) + C_{comm}\left( \tau,d_i \right) + C_{overload}(\tau, t).
\end{equation}

\textit{Вычислительная стоимость} отражает реальные издержки на вычисления:
\begin{equation}
	C_{comp}(\tau) = \rho_{cpu} \cdot \text{CPU}(\tau) + \rho_{mem} \cdot \text{MEM}(\tau),
\end{equation}
где $\rho_{cpu}$ и $\rho_{mem}$ -- удельные стоимости CPU и памяти.

\textit{Коммуникационная стоимость} моделирует <<цену>> передачи данных:
\begin{equation}
	C_{comm}\left( \tau,d_i \right) = 
	\begin{cases}
		w\left( d_i,n_j \right) \cdot DataSize(\tau) \cdot \rho_{net} & \text{если } (d_i,n_j) \in E \\
		dist\left( d_i,n_j \right) \cdot DataSize(t) \cdot \rho_{net} \cdot \gamma & \text{если } (d_i,n_j) \notin E
	\end{cases},
\end{equation}
где $w\left( d_i,n_j \right)$ -- прямой вес ребра между устройством и узлом, $dist\left( d_i,n_j \right)$ -- предполагаемое кратчайшее расстояние в графе, $\rho_{net}$ -- стоимость единицы сетевого трафика, $\gamma > 1$ -- штраф за непрямое соединение.

\textit{Стоимость перегрузки} позволяет учесть не только прямые затраты, но и косвенные эффекты, такие как снижение производительности при высокой загрузке.
\begin{equation}
	C_{overload}(\tau, t) = \rho_{overload} \cdot \max(0, \sum_{\tau} \text{Requirement}(\tau) - \text{Capacity}_j(t))^2.
\end{equation}

\subsection{Фаза 3 -- Определение  платежей (Payment Phase)}\label{sec:ch2/sec2/subsec3}

На третьей фазе агент-аукционера решает оптимальную задачу распределения. Глобальная целевая функция -- \textbf{социальное благополучие (Social Welfare)} -- определяется как сумма всех выигрышей за вычетом затрат:
\begin{equation}
	SW(t) = \sum_{i=1}^{m} \sum_{\tau \in T_i(t)} \sum_{j=1}^{k} x_{i,j}^{\tau}(t) \cdot [U_i(\tau, n_j) - C_j(\tau, d_i)]
\end{equation}
где $x_{i,j}^{\tau}(t) \in \{0, 1\}$ -- индикаторная переменная, указывающая, выполняется ли задача $\tau$ на узле $n_j$ в момент времени $t$.

Максимизация социального благополучия обеспечивает глобальную оптимальность распределения при учёте интересов всех участников~\cite{arrow}.
\begin{equation}
	x^*(t) = \arg\max_{x} SW(t)
\end{equation}

Эта задача может быть решена различными методами:
\begin{itemize}
	\item для малых систем: методы целочисленного программирования;
	\item для больших систем: приближённые алгоритмы (жадные методы, метаэвристики);
	\item с обучением: MARL методы.
\end{itemize}

Центральным элементом решения является \textbf{механизм VCG (Vickrey-Clarke-Groves) платежей}, определяющий сколько должно заплатить устройство, чтобы у агентов не было стимула лгать о своей полезности / стоимости.
\begin{equation}
	p_i(\tau, t) = H_i(\tau) - [{SW}_{- i}(t) - (SW(t) - v_{i}(x_i^{*}))],
\end{equation}
где ${SW}_{- i}$ -- оптимальное социальное благополучие без участия устройства $d_i$, $v_i(x_i^{*})$ -- полученная ценность агентом $d_i$ при оптимальном распределении, $H_i(\tau)$ -- поправка (обычно 0 для простого VCG).

\subsection{Анализ свойств}\label{sec:ch2/sec2/subsec4}

\textbf{Теорема 2.4.1 (Доминантно-стратегическая совместимость -- DSIC):}

Если функции полезности и стоимости истинны, то для каждого устройства $d_i$ при платежах по VCG выполняется:

\begin{equation}
	U_i(\text{истинная заявка}) \geq U_i(\text{любая другая заявка})
\end{equation}

\textit{Доказательство:} Вытекает из свойств VCG механизма.

\textbf{Теорема 2.4.2 (Максимизация социального благосустояния):}

Распределение $x^*$, полученное путём решения задачи оптимизации, максимизирует сумму полезностей всех участников минус сумму затрат.

\textbf{Следствие (Индивидуальная рациональность):}

При правильном выборе $H_i(\tau)$, каждое устройство получает неотрицательную чистую полезность:

\begin{equation}
	U_i(\tau, n_j, t) - p_i(\tau, t) \geq 0
\end{equation}

\section{Алгоритм Dec-POMDP-MARL (QMIX)}\label{sec:ch2/sec3}

Алгоритм MA-VCG обеспечивает справедливое и оптимальное распределение в один момент времени при известных функциях полезности. Однако для полной адаптивности система должна \textbf{учиться} из опыта. \textbf{Dec-POMDP-MARL} обеспечивает эту адаптивность.

\subsection{Dec-POMDP формулировка}\label{sec:ch2/sec3/subsec1}

Задача управления ресурсами может быть переформулирована как децентрализованный частично наблюдаемый марковский процесс принятия решений (\textbf{Dec-POMDP}):

\begin{equation}
	\text{Dec-POMDP} = \langle N, S, A, P, R, O, \Omega, \gamma, h \rangle
\end{equation}

где:
\begin{itemize}
	\item \textbf{Множество агентов:} $N = \{n_1, \ldots, n_k\}$ (edge-узлы);
	
	\item \textbf{Глобальное состояние:} $s \in S$ включает:
	\begin{itemize}
		\item загруженность каждого узла: $\{\text{Load}_j(t)\}_{j=1}^k$;
		\item очередь задач на каждом устройстве: $\{\text{Queue}_i(t)\}_{i=1}^m$;
		\item характеристики сетевой топологии (задержки, пропускная способность);
		\item историю принятых решений.
	\end{itemize}
	
	\item \textbf{Локальное наблюдение агента} $n_j$: $o_j \in O_j$ включает:
	\begin{itemize}
		\item свою локальную загруженность: $\text{Load}_j(t)$;
		\item поступившие запросы от соседних устройств (с шумом);
		\item историю взаимодействий с соседними узлами;
		\item оценку состояния сети (может быть неточной).
	\end{itemize}
	
	\item \textbf{Действия агента} $n_j$: $a_j \in A_j$ -- решение о распределении ресурсов:
	\begin{itemize}
		\item какие задачи принять;
		\item какие отклонить;
		\item на какой приоритет обработки направить.
	\end{itemize}
	Т.е. возможные действия: $A_j = \{\text{Accept}, \text{Reject}, \text{Priority\_High}, \text{Priority\_Low}\}$.
	
	\item \textbf{Функция переходов:} $P(s'|s, a) = P(s'|s, a_1, \ldots, a_k)$ описывает, как состояние системы меняется в результате действий всех агентов;
	
	\item \textbf{Функция локального вознаграждения:} $R_j(s, a)$ для узла $n_j$ в момент времени $t$:	
	\begin{equation}
		R_j(s_t, a_t^j) = \text{Revenue}_j(t) - \text{Cost}_j(t) - P_j(\text{VCG}) - \text{Penalty}_j(t)
	\end{equation}
	где:
	\begin{itemize}
		\item $\text{Revenue}_j = \sum_{\tau \in \text{assigned}} p_i(\tau, t)$ -- сумма платежей от устройств;
		\item $\text{Cost}_j(t) = \sum_{\tau} [C_{\text{comp}}(\tau) + C_{\text{comm}}(\tau)]$ -- затраты;
		\item $P_j(\text{VCG})$ -- платёж узла по VCG;
		\item $\text{Penalty}_j(t)$ -- штраф за нарушение SLA.
	\end{itemize}
	
	\item \textbf{Функция наблюдений:} $\Omega(o|s', a)$ описывает, какое локальное наблюдение получит агент после перехода в новое состояние;
	
	\item \textbf{Коэффициент дисконтирования:} $\gamma \in [0, 1]$ — параметр, определяющий вес будущих вознаграждений;
	
	\item \textbf{Горизонт:} $h$ — количество временных шагов для планирования (обычно переменный в нашем случае).
\end{itemize}

Цель каждого агента -- максимизировать ожидаемое суммарное дисконтированное вознаграждение:
\begin{equation}
	\pi^* = \arg\max_{\pi} \mathbb{E}\left[\sum_{t=0}^{h} \gamma^t R_j(s_t, a_t^j) | \pi\right]
\end{equation}
при условии, что глобальное вознаграждение также стремится к максимизации социального благополучия:
\begin{equation}
	R_{\text{global}}(t) = \text{SW}(t) = \sum_{i=1}^m \sum_{\tau} \sum_{j} x_{i,j}^{\tau}(t) \cdot [U_i(\tau, n_j) - C_j(\tau)]
\end{equation}

\subsection{Компоненты архитектуры QMIX}\label{sec:ch2/sec3/subsec3}

Архитектура QMIX-based системы управления состоит из следующих компонентов:

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{arch-qmix}
	\caption{Концептуальная архитектура QMIX}
	\label{img:arch-qmix}
\end{figure}

\begin{enumerate}
	
	\item \textbf{Локальные Q-сети агентов (Agent Networks):} Каждый Edge-узел $j$ оснащен локальной рекуррентной нейронной сетью (GRU-based), которая обрабатывает локальное наблюдение $o_j^t$ и вычисляет Q-значения для каждого возможного действия $a_j$:
	\begin{equation}
		Q_j(o_j, a_j) = \text{NN}_j(o_j, a_j; \theta_j),
	\end{equation}
	где:
	\begin{itemize}
		\item $o_j$ -- локальное наблюдение узла;
		\item $a_j$ -- действие узла;
		\item $\theta_j$ -- параметры сети узла;
		\item выход -- Q-функция для каждой пары (наблюдение, действие).
	\end{itemize}
	Архитектура нейросети включает GRU слой для запоминания истории, за которым следуют полносвязные слои с ReLU активациями.
	
	\item \textbf{Смешивающая сеть (Mixing Network):} Централизованная сеть, которая объединяет локальные Q-функции $Q_j(o_j, a_j)$ в глобальную Q-функцию:
	\begin{equation}
		Q_{\text{tot}}(s, a) = \sum_{i=1}^k w_i(s) \cdot Q_i(o_i, a_i) + b(s),
	\end{equation}
	где коэффициенты $w_i(s) \geq 0$ и смещение $b(s)$ вычисляются гиперсетью на основе глобального состояния $s$. Использование ReLU активации гарантирует неотрицательность весов и, следовательно, монотонность разложения.

	\textbf{Монотонность:} Смешивающая сеть гарантирует, что увеличение любого локального Q-значения приводит к увеличению глобального Q:
	\begin{equation}
		\frac{\partial Q_{\text{tot}}}{\partial Q_j} \geq 0 \quad \forall j
	\end{equation}
	
	\item \textbf{Целевые сети (Target Networks):} Для стабилизации off-policy обучения используются отдельные target сети как для агентов, так и для Mixing Network:
	\begin{itemize}
		\item $Q_j^{\text{target}}(o_j, a_j; \theta_j^{\text{old}})$;
		\item $Q_{\text{tot}}^{\text{target}}(s, a; \phi^{\text{old}})$.
	\end{itemize}
	Параметры целевых сетей обновляются реже (каждые $\tau_\text{target}$ шагов).
	
	\item \textbf{Experience Replay Buffer:} Хранилище опыта (transition tuples: $o^t, a^t, r^t, o^{t+1}$) для off-policy обучения с максимальным размером переходов (например, $10^4$). 	
	
	\item \textbf{Симулятор окружения (Environment):} Динамическая модель системы, имитирующая прибытие задач (распределение Пуассона), выполнение, завершение, отказы узлов (параметр $\lambda_{\text{failure}} = 0.01$) и варьирование нагрузки (синусоидальное с амплитудой $\pm 30\%$). 
	
\end{enumerate}

\subsection{Ключевые свойства QMIX}\label{sec:ch2/sec3/subsec4}

\textbf{Свойство 1: Монотонность разложения}

Для любых локальных Q-функций $Q_1, \ldots, Q_k$, если архитектура смешивающей сети обеспечивает условие монотонности, то:

\begin{equation}
	\max_{a_1, \ldots, a_k} Q_{\text{tot}}(s, a_1, \ldots, a_k) = Q_{\text{tot}}(s, \arg\max_{a_1} Q_1, \ldots, \arg\max_{a_k} Q_k)
\end{equation}

Это гарантирует, что локальная оптимизация каждого агента приводит к глобальной оптимизации.

\textbf{Свойство 2: Сходимость}

При выполнении условий монотонности алгоритм QMIX сходится к оптимальной политике с вероятностью 1.

\textbf{Свойство 3: Линейная масштабируемость}

\begin{itemize}
	\item \textbf{Сложность обучения:} $O(n \cdot |A|)$ вместо $O(|A|^n)$;
	\item \textbf{Сложность памяти:} $O(n \cdot d)$, где $d$ — размер наблюдения.
\end{itemize}

\subsection{Парадигма CTDE (Centralized Training with Decentralized Execution)}\label{sec:ch2/sec3/subsec2}

В нашем подходе используется специальная парадигма \textbf{CTDE}: централизованное обучение (offline) и децентрализованное исполнение (online).

\textbf{Фаза исполнения (online, децентрализованная):}

Во время исполнения каждый агент $j$ независимо выбирает действие на основе \textbf{$\epsilon$-жадной стратегии стратегии}, основываясь только на локальном наблюдении:
\begin{equation}
	a_j^t = \begin{cases}
		\arg\max_{a_j} Q_j(o_j^t, a_j) & \text{с вероятностью } 1 - \epsilon(t) \\
		\text{случайное действие из } A_j & \text{с вероятностью } \epsilon(t)
	\end{cases}
\end{equation}
где $\epsilon(t) = 0.1 \cdot (1 - t / T_{\text{max}})$ -- линейно убывающий коэффициент исследования.

Поэтому нет необходимости в глобальной коммуникации.

\textbf{Фаза обучения (offline, централизованная):}

Аукционер имеет доступ к глобальному состоянию $s$ и истории взаимодействий; обучаются параметры функций и политика MARL алгоритма. На основе собранного опыта выполняются следующие шаги:

\begin{enumerate}
	
	\item \textbf{Вычисление целевых Q-значений (Double DQN):} Для каждого перехода из experience replay buffer вычисляется целевое значение:
	
	\begin{equation}
		Q_{\text{tot}}^{\text{target}} = r + \gamma \cdot Q_{\text{tot}}^{\text{target}} \left( s', \arg\max_{a'} Q_{\text{tot}}(s', a') \right)
	\end{equation}
	
	Аргмакс берётся по действиям из текущей сети, а значение вычисляется на target сети, что снижает переоценку (overestimation bias).
	
	\item \textbf{Обновление локальных Q-сетей:} Локальные Q-сети обновляются для минимизации потерь:
	
	\begin{equation}
		\mathcal{L}_{\text{agents}} = \mathbb{E}_{(s,a,r,s') \sim \mathcal{B}} \left[ (Q_{\text{tot}}(s, a) - Q_{\text{tot}}^{\text{target}})^2 \right]
	\end{equation}
	
	\item \textbf{Обновление Mixing Network:} Гиперсеть и слои агрегации обновляются с помощью градиентного спуска.
	
	\item \textbf{Мягкое обновление target сетей:} Каждые $\tau_{\text{target}}$ шагов:
	
	\begin{equation}
		\theta_{\text{target}} \leftarrow \alpha \theta + (1 - \alpha) \theta_{\text{target}}, \quad \alpha = 0.01
	\end{equation}
	
\end{enumerate}

\section{Архитектура интегрированной системы управления ресурсами}

\subsection{Общая архитектура системы}

Интегрированная система управления ресурсами состоит из четырёх основных компонентов, работающих циклически: \textbf{Механизм сбора опыта}, \textbf{Блок оценки параметров аукциона}, \textbf{Модуль оптимизации и расчёта платежей} и \textbf{Система обучения агентов}. Эти компоненты взаимодействуют в двух режимах: \textbf{режиме онлайн-исполнения} (Online Phase) и \textbf{режиме оффлайн-обучения} (Offline Phase).

\subsection{Компонент 1: Онлайн-исполнение}

Онлайн-фаза работает в реальном времени, без доступа к глобальному состоянию системы. Каждый edge-узел (агент) принимает решения на основе своего локального наблюдения:
\begin{enumerate}
	\item \textbf{Формирование локальных наблюдений}. В момент времени $t$, каждый узел $n_j$ формирует своё локальное наблюдение $o_j^t$.
	\item \textbf{Вычисление локальной Q-функции}. Каждый узел имеет свою нейросеть, которая вычисляет Q-значения $\mathbf{Q}_j(t)$ для всех возможных действий $a_j$.
	\item \textbf{Выбор действия и исполнение}. Узел выбирает действие на основе $\epsilon$-жадной стратегии, далее либо участвует в аукционе, либо нет.
	\item \textbf{Накопление опыта}. Каждый переход сохраняется в Experience Reply Buffer: $o^t, a^t, r^t, o^{t+1}$.
\end{enumerate}

\subsection{Компонент 2: Оффлайн-обучение}

Оффлай-фаза включает:
\begin{enumerate}
	\item \textbf{Обновление функций полезности и стоимости}. На основе Experience Reply Buffer система обновляет функции полезности и стоимости.
	
	Рассчет полезности устройства $d_i$:
	\begin{equation}
		\hat{U}_i(\tau, n_j) = \frac{1}{N_{ij}} \sum_{k: \text{task } k \text{ assigned to } n_j} U_i(\tau_k, n_j)
	\end{equation}
	
	Рассчет стоимости узла $n_j$:
	\begin{equation}
		\hat{C}_j(\tau, d_i) = \frac{1}{M_j} \sum_{l: \text{task } l \text{ executed on } n_j} C_j(\tau_l, d_i)
	\end{equation}
	
	Гладкое обновление параметров (экспоненциальное скользящее среднее):
	\begin{equation}
		U_i^{\text{new}}(\tau, n_j) = (1 - \alpha_U) \cdot U_i^{\text{old}}(\tau, n_j) + \alpha_U \cdot \hat{U}_i(\tau, n_j)
	\end{equation}
	\begin{equation}
		C_j^{\text{new}}(\tau, d_i) = (1 - \alpha_C) \cdot C_j^{\text{old}}(\tau, d_i) + \alpha_C \cdot \hat{C}_j(\tau, d_i)
	\end{equation}
	
	\item \textbf{Обновление функции вознаграждения}. 
	
	\begin{itemize}
		\item На основе обновленных функций система решает оптимизационную задачу для получения оптимального распределения $x^*$;
		\item Для каждого устройства $d_i$ идет расчет VCG платежей $p_i$;
		\item Рассчитанные платежи интегрируются в функцию вознаграждения $r_j$. 
	\end{itemize}
	
	\item \textbf{Обучение сетей QMIX}. На основе Experience Reply Buffer с обновлёнными вознаграждениями, система обучает сети QMIX путём минимизации TD-ошибки.
	
\end{enumerate}

\subsection{Поток данных: пример со сценарием}

\subsubsection{Время $t=0$ (онлайн-фаза)}

Устройство $d_1$ приходит с задачей $\tau_1$ (CPU=2, важность=0.8). Узлы имеют нагрузку 30\% и 20\% соответственно.

После вычисления Q-функций узлы выбирают действия. Задача $\tau_1$ назначается узлу с максимальным Q-значением для действия Accept. Система получает вознаграждение и сохраняет переход в буфер.

\subsubsection{Оффлайн-фаза (после 1000 переходов)}

Система анализирует буфер и обнаруживает закономерности:
\begin{itemize}
	\item когда узел перегружен (load $> 0.4$), отклонение задач приводит к лучшему результату;
	\item платежи варьируются от 0.5 до 0.8 в зависимости от важности.
\end{itemize}

Обновлённые функции:
\begin{equation}
	\hat{U}_1(\tau, n_1) = 0.65, \quad \hat{U}_1(\tau, n_2) = 0.55
\end{equation}

\begin{equation}
	\hat{C}_1(\tau_1) = 0.30, \quad \hat{C}_1(\tau_2) = 0.35
\end{equation}

После MA-VCG оптимизации и расчёта платежей система переобучает сети QMIX на скорректированных вознаграждениях.

\subsection{Связь между действиями, опытом и параметрами}

Ключевой инвариант системы:

\begin{equation}
	\text{LocalDecision}(a_j) \approx \text{GlobalOptimal}(x^*)
\end{equation}

Локальное решение узла должно быть согласовано с глобально оптимальным распределением. Это достигается через:

\begin{enumerate}
	\item \textbf{QMIX монотонность}: локальная оптимизация $\rightarrow$ глобальная оптимизация;
	\item \textbf{VCG платежи в вознаграждениях}: агенты мотивированы принимать глобально оптимальные решения;
	\item \textbf{Итеративное обучение}: параметры адаптируются к истинным функциям.
\end{enumerate}

Когда этот инвариант выполнен, система одновременно достигает:
\begin{enumerate}
	\item справедливости (гарантии DSIC от VCG);
	\item оптимальности (максимизация социального благосустояния);
	\item адаптивности (обучение из опыта);
	\item масштабируемости ($O(n)$ сложность QMIX);
	\item децентрализации (децентрализованное исполнение).
\end{enumerate}

\section{Выводы главы}

Формализация задачи как Dec-POMDP обеспечивает теоретически строгую модель для анализа многоагентной системы распределения ресурсов. Парадигма CTDE позволяет получить лучшее из обоих миров: мощные оптимизационные методы во время обучения и децентрализованное исполнение в реальной системе. Два предложенных алгоритма (MA-VCG и Dec-POMDP-MARL) дополняют друг друга, обеспечивая справедливость, адаптивность и масштабируемость.

